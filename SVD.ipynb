{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "SVD.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-lOuZ06jzST"
      },
      "source": [
        "# Descomposición de Valor Singular\n",
        "\n",
        "La descomposición de valor singular SVD, es una factorización de matrices reales (o complejas) que generaliza el proceso de diagonalización.\n",
        "\n",
        "Esta factorización es aplicable a cualquier matriz rectangular de $m \\times n$, a diferencia de la descomposición en eigenvalores y eigenvectores, que requieren que $M$ sea cuadrada y que sus eigenvectores generen al espacio completo.\n",
        "\n",
        "La factorización para una matriz $M$ es escrita usualmente como\n",
        "\n",
        "$$\n",
        "M = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "Donde $U$ es una matriz ortogonal de $m \\times m$, $\\Sigma$ es una matriz diagonal de $m \\times n$ con valores no negativos y $V$ es una matriz ortogonal de $n \\times n$. \n",
        "\n",
        "A los valores de la diagonal de $\\Sigma$, los denotamos como $\\sigma_i$, cada uno de ellos es lo que llamaremos, $\\textbf{Valor singular de M}$.\n",
        "\n",
        "Esta factorización no es única. En algunas ocasiones, se suelen acomodar los $\\sigma_i$ en forma descendente, haciendo de esta manera a $\\Sigma$ única pero no necesariamente a $U$ y $V^T$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ATIbyXOjzSj"
      },
      "source": [
        "## Interpretación Geométrica\n",
        "\n",
        "En el caso de que $M$ sea una matriz cuadrada e invertible, se tiene que $U,V$ al ser matrices ortogonales, son el grupo de rotaciones o reflexiones en un $\\mathbb{R}^n$ y la matriz $\\Sigma$ escala cada coordenada por el factor correspondiente en la diagonal.\n",
        "\n",
        "De esta manera, podemos ver que toda transformación lineal invertible, se puede descomponer como una rotación, un escalamiento y otra rotación. \n",
        "\n",
        "Por otro lado, si ocurre que $M$ tiene dimensiones $m \\times n$ es una transformación linela que va de $\\mathbb{R}^n \\longrightarrow \\mathbb{R}^m$. En este caso, tenemos que la matriz $M$, sería la composición de una rotación en $\\mathbb{R}^n$ dada por $V^T$, una escalación y cambio de espacio vectorial a $\\mathbb{R}^m$ y una rotación en $\\mathbb{R}^m$. \n",
        "\n",
        "Notamos que en general, $\\text{rank}(M) \\leq \\min\\{ m,n \\}$, es decir, el máximo rango que puede tener nuestra matriz, es el del menor número ya sea de columnas o renglones.\n",
        "\n",
        "Es decir, $M$ puede mapear un espacio de mayor dimensión a un espacio de menor dimensión $m>n$ o de un espacio de menor dimensión a un espacio de mayor dimensión a uno de menor dimensión $m<n$.\n",
        "\n",
        "Es importante notar que a diferencia de la descomposición espectral de una matriz ($\\mathbf{Diagonalización}$), la factorización en valores singulares puede calcularse incluso en matrices cuyos eigenvectores no formen al espacio completo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGgshkLajzSq"
      },
      "source": [
        "## Relación con las bases\n",
        "\n",
        "Como $U$ es una matriz ortogonal, entonces $U = (U_1, U_2, ..., U_m)$ sus columnas $U_i$ con $1 \\leq i \\leq m$ forman una base ortonormal para $\\mathbb{R}^m$, de la misma manera,\n",
        "\n",
        "$$\n",
        "V^T = \\left( \\begin{array}\\\\\n",
        " V_1^T \\\\\n",
        " V_2^T \\\\\n",
        " \\vdots \\\\\n",
        " V_n^T\n",
        " \\end{array}\n",
        " \\right)\n",
        "$$\n",
        "\n",
        "Los renglones de $V^T$ forman una base ortonormal para $\\mathbb{R}^n$. \n",
        "\n",
        "De esta manera, si $M$ es la matriz asociada a la transformación lineal $T$, se tiene que \n",
        "\n",
        "$$\n",
        "\\begin{array} \\\\\n",
        "T(V_i) = \\sigma_i U_i &  1 \\leq i \\leq \\min\\{ m,n \\}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Es decir, que nuestra matriz $M$, mapea a la base de $\\mathbb{R}^n$ dada por los renglones de $V^T$ a la base de $\\mathbb{R}^m$ dada por las columnas de $U$. \n",
        "\n",
        "En específico, si $m>n$, se tiene que esto ocurrirá para toda la base de $\\mathbb{R}^n$, teniendo así las últimas $(m-n)$ columnas de $U$, poca relevancia para la descomposición de $M$.\n",
        "\n",
        "Por otra parte, si $m<n$, se tiene que los primeros $m$ renglones de $V^T$ mapean por completo a $\\mathbb{R}^m$, por lo que los últimos $(n-m)$ renglones de $V^T$ darán combinaciones lineas de los $U_i$, siendo poco relevantes para la factorización de $M$.\n",
        "\n",
        "Lo anterior se puede ver directamente de la siguiente manera. Suponiendo el caso de $m>n$.\n",
        "\n",
        "$$\n",
        "M V_i = (U_1, U_2, ..., U_m) \\left( \\begin{array}\\\\\n",
        " \\sigma_1 V_1^T \\\\\n",
        " \\sigma_2 V_2^T \\\\\n",
        "  \\vdots \\\\\n",
        " \\sigma_n V_n^T \\\\\n",
        " \\mathbb{O}\n",
        " \\end{array}\n",
        " \\right)\n",
        " V_i\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwCpGouMjzSx"
      },
      "source": [
        "Donde al ser $V^T$ ortogonal, sus renglones serán ortonormales. Además, notamos que $V_i^T V_j = <V_i, V_j>$ que por la condición de ortonormalidad, se tiene que\n",
        "\n",
        "$$\n",
        "<V_i, V_j> = \\delta_{ij} \\left\\{ \\begin{array} \\\\\n",
        " 1 & si & i=j \\\\\n",
        " 0 & si & i \\neq j\n",
        " \\end{array}\n",
        " \\right.\n",
        "$$\n",
        "\n",
        "Por lo que\n",
        "\n",
        "$$\n",
        "MV_i = (U_1, U_2, \\ ... \\ ,\\ U_m) \\sigma_i e_i\n",
        "$$\n",
        "\n",
        "Donde $e_i$ es el i-ésimo vector canónico. Teniendo por último los deseado.\n",
        "\n",
        "$$\n",
        "T(V_i) = M V_i = \\sigma_i U_i\n",
        "$$\n",
        "\n",
        "Notamos que al ser $m>n$, las últimas $(m-n)$ columnas de $U$ no tiene importancia, ya que se anulan con la matriz $\\Sigma$. \n",
        "\n",
        "De esta manera, surge la idea de crear una factorización con matrices singulares reducida, en la cuál se descompone a la matriz $U = (\\hat{U}, \\tilde{U})$ y a la matriz $\\Sigma$ como\n",
        "\n",
        "$$\n",
        "\\Sigma =  \\left( \\begin{array}\\\\ \\hat{\\Sigma}\\\\ \\tilde{\\Sigma} \\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Siendo nuestra descomposición reducida\n",
        "\n",
        "$$\n",
        "M = \\hat{U} \\hat{\\Sigma} V^T\n",
        "$$\n",
        "\n",
        "\n",
        "Notamos que la matriz $\\hat{U}$ es de $m \\times n$, la matriz $\\hat{\\Sigma}$ es de $n \\times n$ regresando aún una matriz de $m \\times n$, pero se ha reducido la cantidad de columnas y renglones que carecían de importancia, manteniendo así, toda la información de $M$ pero con exactamente la información necesaria.\n",
        "\n",
        "Para el caso de $m<n$, se consigue algo similar, descomponiendo a $\\Sigma = (\\hat{\\Sigma} \\tilde{\\Sigma})$ y a $V^T$ como\n",
        "\n",
        "$$\n",
        "V^T =  \\left( \\begin{array}\\\\ \\hat{V}^T\\\\ \\tilde{V}^T \\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Quedando la factorización\n",
        "\n",
        "$$\n",
        "M= U \\hat{\\Sigma} \\hat{V}^T\n",
        "$$\n",
        "\n",
        "Al igual que en el caso anterior, se mantiene toda la información de $M$ pero en matrices con menores dimensiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV0hpdjojzSz"
      },
      "source": [
        "## Prueba de la existencia de la factorización\n",
        "\n",
        "Hasta ahora hemos hablado de las propiedades e interpretaciones de la factorización de nuestra matriz $M$, sin embargo hemos asumido su existencia, por lo que ahora probaremos esto y la misma prueba nos dará un algoritmo para su obtención\n",
        "\n",
        "Probaremos primero que dada una matriz de $M$ de $m \\times n$ arbitraria, la matriz\n",
        "\n",
        "$$\n",
        "X = M^T M\n",
        "$$\n",
        "\n",
        "Es una matriz de simétrica\n",
        "\n",
        "Sean $[M]_{ij} = M_{ij}$ los elementos de $M$. Entonces por multiplicación de matrices, tenemos que\n",
        "\n",
        "$$\n",
        "[X]_{ij} = \\sum_{k=1}^m [M^T]_{ik}[M]_{kj} = \\sum_{k=1}^m M_{ki} M_{kj} \\\\\n",
        "[X]_{ji} = \\sum_{k=1}^m [M^T]_{jk}[M]_{ki} = \\sum_{k=1}^m M_{kj}M_{ki}\\\\\n",
        "\\Rightarrow [X]_{ij} = [X]_{ji}\n",
        "$$\n",
        "\n",
        "Por lo que $X$ es una matriz simétrica, $X^T = X$.\n",
        "\n",
        "Ahora, utilizando el $\\textbf{Teorema de Descomposición Espectral}$, sabemos que existe una base de eigenvectores de $X$ que genera al espacio completo, es decir, que representando a esta matriz en la base de eigenvectores, podemos encontrar una matriz diagonal la cuál consiste en los vectores propios de $X$. Sea\n",
        "\n",
        "$$\n",
        "V=(V_1, V_2,\\ ...\\ , V_n)\n",
        "$$\n",
        "\n",
        "Donde $V_i$ es el i-ésimo eigenvector de $X$. De esta manera, se tiene que \n",
        "\n",
        "\n",
        "$$\n",
        "V^T X V = D = \\left( \\begin{array}\\\\ \\hat{D} & 0\\\\ 0 & 0 \\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Donde $\\hat{D}$ es una matriz diagonal de $r \\times r$ cuyas entradas son los eigenvalores distintos de cero de $X$. En general, $r \\leq \\min \\{ m,n \\}$. \n",
        "\n",
        "Al igual que anteriormente, la existencia de ceros en nuestra matriz diagonal, nos permite reducir la expresión de la siguiente manera. \n",
        "\n",
        "$$\n",
        "\\left( \\begin{array}\\\\\n",
        "V_1^T \\\\\n",
        "V_2 ^T\n",
        "\\end{array} \\right)\n",
        "X \\left(\\begin{array}\\\\ V_1 & V_2\\end{array} \\right) = \n",
        "\\left( \\begin{array}\\\\\n",
        " V_1^T X V_1 & V_1^T X V_2 \\\\\n",
        " V_2^T X V_1 & V_2^T X V_2\n",
        " \\end{array} \\right) = \\left( \\begin{array}\\\\ \\hat{D} & 0\\\\ 0 & 0 \\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Notamos entonces que en particular $V_1^T X V_1 = D$.\n",
        "\n",
        "Definimos a $[D^{\\pm 1/2}]_{ij} = \\left\\{ \\begin{array}\\\\ (\\sqrt{D_{ii}})^{\\pm 1} & si  & i=j \\\\\n",
        "0 & si & i \\neq j \\end{array} \\right.$\n",
        "\n",
        "Y definimos también\n",
        "\n",
        "$$\n",
        "U_1 = M V_1 D^{-1/2}\n",
        "$$\n",
        "\n",
        "Notamos que $D^{-1/2} D^{1/2}= \\text{Id}_{r \\times r}$, y que nuestra matriz $U_1$ tendrá dimensiones de $m \\times r$. Así, se tiene que\n",
        "\n",
        "$$\n",
        "U_1 D^{1/2} V_1^T = (M V_1 D^{-1/2}) D^{1/2}V_1^T = M V_1 V_1^T = M\n",
        "$$\n",
        "\n",
        "Ahora, como $r \\leq m$, para formar una base de ortonormal de $\\mathbb{R}^m$ hacen falta $(m-r)$ vectores ortonormales a los obtenidos en $U_1$, que puede verificarse, es una matriz ortogonal. Escogemos $U_2$ con estos vectores ortonormales que completen la base de tal forma que\n",
        "\n",
        "$$U = (U_1\\ \\ U_2)$$\n",
        "\n",
        "Sea ortogonal, de esta manera. tenemos que $U$ es una matriz de $m \\times m$. La dimensión de $U_2$ será $m \\times (m-r)$.\n",
        "\n",
        "De la misma manera, definimos a $\\Sigma$ \n",
        "\n",
        "$$\n",
        "\\Sigma = \\left( \\begin{array}\\\\\n",
        "  \\left(\\begin{array}\\\\\n",
        "  D^{1/2} & 0 \\\\\n",
        "  0 & 0 \n",
        "  \\end{array}\\right) \\\\\n",
        " 0\n",
        " \\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Donde el cero de abajo tendrá una dimensión de $(m-r) \\times n$ y la parte de arriba será de $r \\times n$, siendo su dimensión en general de $m \\times n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq4wUBLMjzS0"
      },
      "source": [
        "De esta manera, al final tenemos que \n",
        "\n",
        "$$\n",
        "( U_1 \\ \\ U_2) \\left( \\begin{array}\\\\\n",
        "  \\left(\\begin{array}\\\\\n",
        "  D^{1/2} & 0 \\\\\n",
        "  0 & 0 \n",
        "  \\end{array}\\right) \\\\\n",
        " 0\n",
        " \\end{array} \\right)\n",
        "\\left( \\begin{array}\\\\ V_1^T \\\\ V_2^T \\end{array} \\right) = \n",
        "(U_1\\ \\ U_2) \\left( \\begin{array}\\\\ D^{1/2} V_1^T\\\\ 0 \\end{array} \\right)\n",
        "= U_1 D^{1/2}V_1^T = M\n",
        "$$\n",
        "\n",
        "Obteniendo así la factorización deseada\n",
        "\n",
        "$$\n",
        "M = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "Es inmediato que de esta forma la matriz $M M^T$ tiene como eigenvectores las columnas de $U$, ya que\n",
        "\n",
        "$$\n",
        "M M^T = (U \\Sigma V^T)\\  (V \\Sigma U^T) = U \\Sigma \\Sigma U^T = UDU^T\n",
        "$$\n",
        "\n",
        "Siendo $D$ la misma matriz obtenida anteriormente, por lo que $X = M^T M$ y $x^T$ comparten eigenvalores, los eigenvectores de $X$ son las columnas de $V$ y los eigenvalores de $X^T$ son las columnas de $U$. $U,V$ también son conocidos como eigenvectores izquierdo y derecho respectivamente y su relación con diagonalización de $X$ es que si $\\lambda_i$ es un eigenvalor de $X$, entonces \n",
        "\n",
        "$$\n",
        "\\sigma_i = \\sqrt{\\lambda_i}\n",
        "$$\n",
        "\n",
        "Es un valor singular de $M$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b8Shj6-jzS1"
      },
      "source": [
        "## Ejemplo\n",
        "\n",
        "Como dijimos al principio, esta factorización tiene ventajas sobre la factorización en eigenvectores, ya que esta factorización está disponible para todas las matrices, incluso para aquellas no diagonalizables.\n",
        "\n",
        "Definamos \n",
        "\n",
        "$$\n",
        "M = \\left( \\begin{array}\\\\\n",
        " 0 & -1 \\\\\n",
        " 1 & 0\n",
        "\\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Al querer calcular los eigenvalores de $A$, se tiene que\n",
        "\n",
        "$$\n",
        "p(\\lambda) = \\lambda^2 +1\n",
        "$$\n",
        "\n",
        "De esta manera, al trabajar únicamente en $\\mathbb{R}^2$, no hay eigenvalores en nuestro campo, por lo que no podemos encontrar una factorización de eigenvectores en $\\mathbb{R}^{2 \\times 2}$. \n",
        "\n",
        "Ahora, calculemos con el procedimiento anterior la factorización de $M$. \n",
        "\n",
        "Notamos que $X = M^T M = \\text{Id}$ Así, inmediatamente, se tiene \n",
        "\n",
        "$$\n",
        "X = (e_1\\ \\ e_2) \\text{Id} (e_1\\ \\ e_2)^T\n",
        "$$\n",
        "\n",
        "Por lo que $V =( e_1, e_2)$. Así, definiendo a $U$ como \n",
        "\n",
        "$$\n",
        "U = M V D^{-1/2} = M \\text{Id}\\ \\text{Id} = M\n",
        "$$\n",
        "\n",
        "Por lo que la factorización en de $M$ es\n",
        "\n",
        "$$\n",
        "M = \\left( \\begin{array}\\\\\n",
        " 0 & -1 \\\\\n",
        " 1 & 0\n",
        " \\end{array} \\right)\n",
        " \\left( \\begin{array}\\\\\n",
        " 1 & 0 \\\\\n",
        " 0 & 1\n",
        " \\end{array} \\right)\n",
        " \\left( \\begin{array}\\\\\n",
        " 1 & 0 \\\\\n",
        " 0 & 1\n",
        " \\end{array} \\right)\n",
        "$$\n",
        "\n",
        "Notamos que claramente, $U$, $V$ son ortogonales y $\\Sigma$ es diagonal con los eigenvalores como elementos.\n",
        "\n",
        "Para entender un poco mejor esto, veamos que nuestra matriz $M$, lo que realiza es una rotación de $\\pi/2$ en el sentido positivo, por lo que no existe una dirección que deje invariante, así, no hay eigenvectores en $\\mathbb{R}^2$ de esta matriz, sin embargo, supongamos lo siguiente.\n",
        "\n",
        "Sea $V$ el espacio vectorial $\\mathbb{R}^2$ con la base canónica y sea $W$ el espacio vectorial $\\mathbb{R}^2$ con la base $\\beta$\n",
        "\n",
        "$$\n",
        "\\beta= \\{ (0,1)^T, (-1,0)^T \\}\n",
        "$$\n",
        "\n",
        "Y sea $T$ la transformación lineal tal que \n",
        "\n",
        "$$\n",
        "T(v) = Mv\n",
        "$$\n",
        "\n",
        "De esta manera, la representación matricial de $T$ en las base canónica y $\\beta$, será diagonal. Más aún, tendrá los valores singulares de $M$ en la diagoal misma.\n",
        "\n",
        "Es decir, esta factorización nos permite dotar a nuestros espacio vectoriales de bases distintas de tal forma que en estas, la representación matricial de una tranformación lineal sea diagonal, a diferencia de la factorización de eigenvectores que únicamente nos deja dotar al mismo espacio de una sola base y es válido solo para transformaciones que van del mismo espacio, al mismo espacio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbjeR3yLjzS3"
      },
      "source": [
        "## Matriz Pseudoinversa\n",
        "\n",
        "Ahora, utilizaremos la factorización para obtener una matriz pseudo inversa de nuestra matriz $M$. \n",
        "\n",
        "Primero definiremos qué es lo que debe cumplir la matriz pseudo-inversa. A la matriz pseudoinversa de $A$, la denotaremos como $A^+$\n",
        "\n",
        "1) $AA^{+}A = A$\n",
        "\n",
        "2) $A^+ A A^+ = A^+$\n",
        "\n",
        "3) $(AA^+)^T = AA^+$\n",
        "\n",
        "4) $(A^+A)^T = A^+ A$\n",
        "\n",
        "Ahora, siendo $M$ una matriz de $m \\times n$ arbitraria, y siendo\n",
        "\n",
        "$$\n",
        "M = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "La descomposición en valores singulares de $M$, definimos la pseudoinversa de $M$ como\n",
        "\n",
        "$$\n",
        "M^+ = V \\Sigma^{+} U^T\n",
        "$$\n",
        "\n",
        "Donde $\\Sigma^{+}$ será la matriz diagonal que tenga como entradas a los inversos de los valores singulares distintos de cero.\n",
        "\n",
        "Notamos que\n",
        "\n",
        "1) $ M M^+ M = U \\Sigma V^T V \\Sigma^+ U^T U \\Sigma V^T = U \\Sigma \\Sigma^+ \\Sigma V^T = U \\Sigma V^T = M$\n",
        "\n",
        "Utilizando que  $U,V$ son matrices ortogonales, y que $\\Sigma \\Sigma^+ = \\Sigma^+ \\Sigma$ y que el resultado, es una matriz que contiene una identidad de la misma dimensión que la matriz cuadrada de valores singulares, por lo que\n",
        "\n",
        "$$\n",
        "\\Sigma \\Sigma^+ \\Sigma = \\Sigma\n",
        "$$\n",
        "\n",
        "Con la explicación anterior, la verificación de las otras propiedades resulta similar, así que definida de esta forma $M^+$, podemos afirmar que $M^+$ es la matriz pseudoinversa de $M$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mN2JMhFjzS7"
      },
      "source": [
        "$M(e_1) = e_2$\n",
        "\n",
        "$M(e_2) = (-1,0)^T$\n",
        "\n",
        "$$\n",
        "Mx=b\n",
        "$$\n",
        "\n",
        "$$\n",
        "x_0=M^+b\n",
        "$$\n",
        "\n",
        "\n",
        "$x_0$ minimiza lo siguiente\n",
        "\n",
        "$$\n",
        "||Mx - b ||\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTbtgaIRjzS8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}